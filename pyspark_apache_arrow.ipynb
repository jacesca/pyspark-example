{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f82ee3bc-2a1c-49ef-aa29-9f3b4ddfb048",
   "metadata": {},
   "source": [
    "# PySpark: Apache Arrow in PySpark\n",
    "\n",
    "Apache Arrow is an in-memory columnar data format that is used in Spark to efficiently transfer data between JVM and Python processes. This currently is most beneficial to Python users that work with Pandas/NumPy data. Its usage is not automatic and might require some minor changes to configuration or code to take full advantage and ensure compatibility. This guide will give a high-level description of how to use Arrow in Spark and highlight any differences when working with Arrow-enabled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0dd622-236e-44d7-8813-bffa94e55daa",
   "metadata": {},
   "source": [
    "## Ensure PyArrow Installed\n",
    "\n",
    "> pip install pyspark[sql]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47cfe5f-e34e-4220-920e-abbab5758cc1",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b97ef73-6625-40b9-97e2-2a04b0151933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Iterator, Tuple\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import pandas_udf, col, udf\n",
    "from pyspark.sql.types import LongType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5d5928-d07c-4929-bb42-0f025e531dc8",
   "metadata": {},
   "source": [
    "## Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "001848b3-4016-46e5-8e61-a2f92a752587",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef5eb5ba-31dc-4036-9df7-126516040a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable spark.sql.repl.eagerEval.enabled configuration for the eager evaluation of PySpark DataFrame in notebooks\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fa1941-d54b-4d64-b421-ffc1a0b1d0dd",
   "metadata": {},
   "source": [
    "## Enabling for Conversion to/from Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82879bea-b13e-4651-96a6-974ccb7227ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>0</th><th>1</th><th>2</th></tr>\n",
       "<tr><td>0.09874711148286308</td><td>0.893175805710203</td><td>0.04764430915487283</td></tr>\n",
       "<tr><td>0.9567366946948238</td><td>0.3398680130169368</td><td>0.6447115632156324</td></tr>\n",
       "<tr><td>0.03308413864204285</td><td>0.16093026686912182</td><td>0.5050091336423522</td></tr>\n",
       "<tr><td>0.7367932835868325</td><td>0.7755342205961568</td><td>0.9842201067808896</td></tr>\n",
       "<tr><td>0.36891693043383345</td><td>0.704545853193771</td><td>0.2697110625143786</td></tr>\n",
       "<tr><td>0.3322482487964862</td><td>0.47244194524276584</td><td>0.7760579952377767</td></tr>\n",
       "<tr><td>0.12842184124849287</td><td>0.031405342495014144</td><td>0.9252385775188006</td></tr>\n",
       "<tr><td>0.3784426798669348</td><td>0.03358823579743109</td><td>0.9923117758080666</td></tr>\n",
       "<tr><td>0.8789964656028638</td><td>0.21192593336414312</td><td>0.33882249740779957</td></tr>\n",
       "<tr><td>0.6220994893016789</td><td>0.6239021274755167</td><td>0.6232632692887881</td></tr>\n",
       "<tr><td>0.7029232389392991</td><td>0.9142050653471363</td><td>0.35539899649183393</td></tr>\n",
       "<tr><td>0.6056754834626187</td><td>0.7866702204041475</td><td>0.8110986615012193</td></tr>\n",
       "<tr><td>0.3729271191685142</td><td>0.6624388580468329</td><td>0.029673834803349286</td></tr>\n",
       "<tr><td>0.8775884494337232</td><td>0.19798132389315082</td><td>0.5575867541199727</td></tr>\n",
       "<tr><td>0.2963926641830119</td><td>0.47336738840185866</td><td>0.26811380284020303</td></tr>\n",
       "<tr><td>0.11530125761959331</td><td>0.04830357616636671</td><td>0.7370992418258543</td></tr>\n",
       "<tr><td>0.012886081660293813</td><td>0.033832047334086846</td><td>0.4258514792290754</td></tr>\n",
       "<tr><td>0.27555400564722055</td><td>0.5676204874321132</td><td>0.3298245956022168</td></tr>\n",
       "<tr><td>0.8831646964657697</td><td>0.6115902450893649</td><td>0.8658867340614997</td></tr>\n",
       "<tr><td>0.4165585215929959</td><td>0.16159770105786508</td><td>0.6798116743669026</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+--------------------+--------------------+\n",
       "|                   0|                   1|                   2|\n",
       "+--------------------+--------------------+--------------------+\n",
       "| 0.09874711148286308|   0.893175805710203| 0.04764430915487283|\n",
       "|  0.9567366946948238|  0.3398680130169368|  0.6447115632156324|\n",
       "| 0.03308413864204285| 0.16093026686912182|  0.5050091336423522|\n",
       "|  0.7367932835868325|  0.7755342205961568|  0.9842201067808896|\n",
       "| 0.36891693043383345|   0.704545853193771|  0.2697110625143786|\n",
       "|  0.3322482487964862| 0.47244194524276584|  0.7760579952377767|\n",
       "| 0.12842184124849287|0.031405342495014144|  0.9252385775188006|\n",
       "|  0.3784426798669348| 0.03358823579743109|  0.9923117758080666|\n",
       "|  0.8789964656028638| 0.21192593336414312| 0.33882249740779957|\n",
       "|  0.6220994893016789|  0.6239021274755167|  0.6232632692887881|\n",
       "|  0.7029232389392991|  0.9142050653471363| 0.35539899649183393|\n",
       "|  0.6056754834626187|  0.7866702204041475|  0.8110986615012193|\n",
       "|  0.3729271191685142|  0.6624388580468329|0.029673834803349286|\n",
       "|  0.8775884494337232| 0.19798132389315082|  0.5575867541199727|\n",
       "|  0.2963926641830119| 0.47336738840185866| 0.26811380284020303|\n",
       "| 0.11530125761959331| 0.04830357616636671|  0.7370992418258543|\n",
       "|0.012886081660293813|0.033832047334086846|  0.4258514792290754|\n",
       "| 0.27555400564722055|  0.5676204874321132|  0.3298245956022168|\n",
       "|  0.8831646964657697|  0.6115902450893649|  0.8658867340614997|\n",
       "|  0.4165585215929959| 0.16159770105786508|  0.6798116743669026|\n",
       "+--------------------+--------------------+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Generate a Pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "print((df.count(), len(df.columns)))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d688cb99-849f-425a-b8b9-3fb0fa4d31b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas DataFrame result statistics:\n",
      "                0           1           2\n",
      "count  100.000000  100.000000  100.000000\n",
      "mean     0.462936    0.458167    0.497645\n",
      "std      0.281031    0.306636    0.288140\n",
      "min      0.012886    0.005309    0.001311\n",
      "25%      0.243387    0.173176    0.269334\n",
      "50%      0.411746    0.401460    0.434134\n",
      "75%      0.711391    0.718346    0.777430\n",
      "max      0.965285    0.994722    0.993816\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()\n",
    "\n",
    "print(\"Pandas DataFrame result statistics:\\n%s\\n\" % str(result_pdf.describe()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08d4879-636e-4d41-bde0-800a2c03d049",
   "metadata": {},
   "source": [
    "## Pandas UDFs (a.k.a. Vectorized UDFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44405f21-416e-482b-96b0-a9baa21e43db",
   "metadata": {},
   "source": [
    "### Series to Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dccbdc8f-70fe-452e-9756-51e0b576bd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    4\n",
      "2    9\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Declare the function and create the UDF\n",
    "def multiply_func(a: pd.Series, b: pd.Series) -> pd.Series:\n",
    "    return a * b\n",
    "\n",
    "multiply = pandas_udf(multiply_func, returnType=LongType())  # type: ignore[call-overload]\n",
    "\n",
    "# The function for a pandas_udf should be able to execute with local Pandas data\n",
    "x = pd.Series([1, 2, 3])\n",
    "print(multiply_func(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6a9e52a-6603-412a-80df-d10aaa7d6dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|multiply_func(x, x)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  4|\n",
      "|                  9|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark DataFrame, 'spark' is an existing SparkSession\n",
    "df = spark.createDataFrame(pd.DataFrame(x, columns=[\"x\"]))\n",
    "\n",
    "# Execute function as a Spark vectorized UDF\n",
    "df.select(multiply(col(\"x\"), col(\"x\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755151ad-8ff3-48cb-aef3-e4e225c79910",
   "metadata": {},
   "source": [
    "### Iterator of Series to Iterator of Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e815db6-63ba-4493-aa7e-ae9d2643be62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf = pd.DataFrame([1, 2, 3], columns=[\"x\"])\n",
    "df = spark.createDataFrame(pdf)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "861e1682-6974-4d9d-9b14-3e6fdf0d29d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|plus_one(x)|\n",
      "+-----------+\n",
      "|          2|\n",
      "|          3|\n",
      "|          4|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Declare the function and create the UDF\n",
    "@pandas_udf(\"long\")  # type: ignore[call-overload]\n",
    "def plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    for x in iterator:\n",
    "        yield x + 1\n",
    "\n",
    "df.select(plus_one(\"x\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e80bf45-4773-47d9-87b9-688e27bf7e95",
   "metadata": {},
   "source": [
    "### Iterator of Multiple Series to Iterator of Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9edd6d7e-85d3-426c-862b-9b447e08850c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>x</th><th>y</th></tr>\n",
       "<tr><td>1</td><td>1</td></tr>\n",
       "<tr><td>2</td><td>4</td></tr>\n",
       "<tr><td>3</td><td>9</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---+\n",
       "|  x|  y|\n",
       "+---+---+\n",
       "|  1|  1|\n",
       "|  2|  4|\n",
       "|  3|  9|\n",
       "+---+---+"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = pd.DataFrame([(1, 1), (2, 4), (3, 9)], columns=[\"x\", \"y\"])\n",
    "df = spark.createDataFrame(pdf)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1b8d637-de3f-4bae-a467-b58442a7be77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|multiply_two_cols(x, y)|\n",
      "+-----------------------+\n",
      "|                      1|\n",
      "|                      8|\n",
      "|                     27|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Declare the function and create the UDF\n",
    "@pandas_udf(\"long\")  # type: ignore[call-overload]\n",
    "def multiply_two_cols(\n",
    "        iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    for a, b in iterator:\n",
    "        yield a * b\n",
    "\n",
    "df.select(multiply_two_cols(\"x\", \"y\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2122b87d-8dc5-4879-8ffd-78ba7cb3bfa9",
   "metadata": {},
   "source": [
    "### Series to Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2675143-3eb3-4945-89c5-f66f28864e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>v</th></tr>\n",
       "<tr><td>1</td><td>1.0</td></tr>\n",
       "<tr><td>1</td><td>2.0</td></tr>\n",
       "<tr><td>2</td><td>3.0</td></tr>\n",
       "<tr><td>2</td><td>5.0</td></tr>\n",
       "<tr><td>2</td><td>10.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+----+\n",
       "| id|   v|\n",
       "+---+----+\n",
       "|  1| 1.0|\n",
       "|  1| 2.0|\n",
       "|  2| 3.0|\n",
       "|  2| 5.0|\n",
       "|  2|10.0|\n",
       "+---+----+"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\")\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce06f036-9d99-47cd-94a4-df6e5ec6b059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|mean_udf(v)|\n",
      "+-----------+\n",
      "|        4.2|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Declare the function and create the UDF\n",
    "@pandas_udf(\"double\")  # type: ignore[call-overload]\n",
    "def mean_udf(v: pd.Series) -> float:\n",
    "    return v.mean()\n",
    "\n",
    "df.select(mean_udf(df['v'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6206b12e-a7e8-4192-aa43-7e6755c17cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|mean_udf(v)|\n",
      "+---+-----------+\n",
      "|  1|        1.5|\n",
      "|  2|        6.0|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"id\").agg(mean_udf(df['v'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d87362a8-4855-4b37-a44f-7c029af50dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|   v|mean_v|\n",
      "+---+----+------+\n",
      "|  1| 1.0|   1.5|\n",
      "|  1| 2.0|   1.5|\n",
      "|  2| 3.0|   6.0|\n",
      "|  2| 5.0|   6.0|\n",
      "|  2|10.0|   6.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = (Window.partitionBy('id')\n",
    "           .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing))\n",
    "df.withColumn('mean_v', mean_udf(df['v']).over(w)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa89c15a-6826-47c9-acfc-c4ed0601f955",
   "metadata": {},
   "source": [
    "## Pandas Function APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c974cad2-4332-4bdf-bdbb-3b66fe2dd72c",
   "metadata": {},
   "source": [
    "### Grouped Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "388de52e-c4b1-47c0-a31a-ed763731db67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1| 1.0|\n",
      "|  1| 2.0|\n",
      "|  2| 3.0|\n",
      "|  2| 5.0|\n",
      "|  2|10.0|\n",
      "+---+----+\n",
      "\n",
      "+---+-----------+\n",
      "| id|mean_udf(v)|\n",
      "+---+-----------+\n",
      "|  1|        1.5|\n",
      "|  2|        6.0|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))\n",
    "df.show()\n",
    "\n",
    "df.groupby(\"id\").agg(mean_udf(df['v'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebfaf455-7377-4c51-b201-5a0de84998a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1|-0.5|\n",
      "|  1| 0.5|\n",
      "|  2|-3.0|\n",
      "|  2|-1.0|\n",
      "|  2| 4.0|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def subtract_mean(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    # pdf is a pandas.DataFrame\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=v - v.mean())\n",
    "\n",
    "df.groupby(\"id\").applyInPandas(subtract_mean, schema=\"id long, v double\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d11cde0-cbe0-4b33-9888-03d888c790bd",
   "metadata": {},
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d5cbd7b-3f93-4f24-aaac-8b5b3398abef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1| 21|\n",
      "|  2| 30|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a5f1e6d-12cd-4ebb-b6ea-4d58064c0557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1| 21|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def filter_func(iterator: Iterable[pd.DataFrame]) -> Iterable[pd.DataFrame]:\n",
    "    for pdf in iterator:\n",
    "        yield pdf[pdf.id == 1]\n",
    "\n",
    "df.mapInPandas(filter_func, schema=df.schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b59c7-f3f7-4400-b2ac-1f629ff63a13",
   "metadata": {},
   "source": [
    "### Co-grouped Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad426470-aa87-454b-a75a-0a1bb3320077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+\n",
      "|    time| id| v1|\n",
      "+--------+---+---+\n",
      "|20000101|  1|1.0|\n",
      "|20000101|  2|2.0|\n",
      "|20000102|  1|3.0|\n",
      "|20000102|  2|4.0|\n",
      "+--------+---+---+\n",
      "\n",
      "+--------+---+---+\n",
      "|    time| id| v2|\n",
      "+--------+---+---+\n",
      "|20000101|  1|  x|\n",
      "|20000101|  2|  y|\n",
      "+--------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(\n",
    "    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
    "    (\"time\", \"id\", \"v1\"))\n",
    "\n",
    "df2 = spark.createDataFrame(\n",
    "    [(20000101, 1, \"x\"), (20000101, 2, \"y\")],\n",
    "    (\"time\", \"id\", \"v2\"))\n",
    "\n",
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac501b86-87ab-4ec8-8791-44a3d1b72bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+----+\n",
      "|    time| id| v1|  v2|\n",
      "+--------+---+---+----+\n",
      "|20000101|  1|1.0|   x|\n",
      "|20000102|  1|3.0|NULL|\n",
      "|20000101|  2|2.0|   y|\n",
      "|20000102|  2|4.0|NULL|\n",
      "+--------+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def merge_ordered(left: pd.DataFrame, right: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.merge_ordered(left, right)\n",
    "\n",
    "(df1.groupby(\"id\")\n",
    "    .cogroup(df2.groupby(\"id\"))\n",
    "    .applyInPandas(merge_ordered, schema=\"time int, id int, v1 double, v2 string\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec4d39-1e3d-421f-8c7c-451eb92fef44",
   "metadata": {},
   "source": [
    "## Arrow Python UDFs\n",
    "\n",
    "Arrow Python UDFs are user defined functions that are executed row-by-row, utilizing Arrow for efficient batch data transfer and serialization. To define an Arrow Python UDF, you can use the udf() decorator or wrap the function with the udf() method, ensuring the useArrow parameter is set to True. Additionally, you can enable Arrow optimization for Python UDFs throughout the entire SparkSession by setting the Spark configuration spark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88024ade-2172-4f55-9304-dc4161037e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+\n",
      "| id|    name|age|\n",
      "+---+--------+---+\n",
      "|  1|John Doe| 21|\n",
      "+---+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7072e61-6acf-4668-aeb3-86ff5e80bed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|slen(name)|arrow_slen(name)|\n",
      "+----------+----------------+\n",
      "|         8|               8|\n",
      "+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@udf(returnType='int')  # A default, pickled Python UDF\n",
    "def slen(s):  # type: ignore[no-untyped-def]\n",
    "    return len(s)\n",
    "\n",
    "@udf(returnType='int', useArrow=True)  # An Arrow Python UDF\n",
    "def arrow_slen(s):  # type: ignore[no-untyped-def]\n",
    "    return len(s)\n",
    "\n",
    "df.select(slen(\"name\"), arrow_slen(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2dfe6f-2345-4569-8fcd-efb8037c1956",
   "metadata": {},
   "source": [
    "## Close session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "385e4838-11ca-4bb1-a147-11fb015ecb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e1bd0-12ed-4979-951d-0838abdddd8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
